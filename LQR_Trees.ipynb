{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Modern Control Paradigms**\n",
    "### **LQR - Trees**\n",
    "#### **Feedback Motion Planning via Sums-of-Squares Verification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda \n",
    "* **Feedback motion planning algorithm** which uses computed stability regions to build a **sparse tree of LQR-stabilized trajectories**\n",
    "\n",
    "* Linear Feedback Design and Verification\n",
    "\n",
    "* LQR-Tree Algorithm\n",
    "\n",
    "* Limitations of LQR-Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ### Direct computation of Lyapunov functions\n",
    "\n",
    "* ### Feedback Synthesis by Sums-of-Squares Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Feedback Design and Verification\n",
    "\n",
    "## Stabilizing a Goal State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Consider a smooth nonlinear system\n",
    "\n",
    "$$\n",
    "\\dot{\\mathbf{x}}=\\mathbf{f}(\\mathbf{x}, \\mathbf{u}) \\text {. }\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^n$ is the state of the system and $\\mathbf{u} \\in \\mathbb{R}^m$ is the control input. We first examine stabilizing a goal-state with an infinite horizon LQR controller, and approximating the closed loop region of attraction. Consider a goal state, $\\mathbf{x}_G$, with $\\mathbf{u}_G$ defined so that $\\mathbf{f}\\left(\\mathbf{x}_G, \\mathbf{u}_G\\right)=0$. Define\n",
    "$$\n",
    "\\overline{\\mathbf{x}}=\\mathbf{x}-\\mathbf{x}_G, \\quad \\overline{\\mathbf{u}}=\\mathbf{u}-\\mathbf{u}_G .\n",
    "$$\n",
    "Now, linearize the system around $\\left(\\mathbf{x}_G, \\mathbf{u}_G\\right)$ to yield the dynamics:\n",
    "$$\n",
    "\\overline{\\mathbf{x}}(t) \\approx \\mathbf{A} \\overline{\\mathbf{x}}(t)+\\mathbf{B} \\overline{\\mathbf{u}}(t) .\n",
    "$$\n",
    "We assume that this linearization of $\\mathbf{f}$ about the goal is controllable. Define the quadratic regulator cost-to-go function as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J\\left(\\overline{\\mathbf{x}}^{\\prime}\\right) & =\\int_0^{\\infty}\\left[\\overline{\\mathbf{x}}^T(t) \\mathbf{Q} \\overline{\\mathbf{x}}(t)+\\overline{\\mathbf{u}}^T(t) \\mathbf{R} \\overline{\\mathbf{u}}(t)\\right] d t \\\\\n",
    "\\mathbf{Q} & =\\mathbf{Q}^T \\geq 0, \\mathbf{R}=\\mathbf{R}^T>0, \\overline{\\mathbf{x}}(0)=\\overline{\\mathbf{x}}^{\\prime}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The optimal cost-to-go function for the linear system is given by\n",
    "$$\n",
    "J^*(\\overline{\\mathbf{x}})=\\overline{\\mathbf{x}}^T \\mathbf{S} \\overline{\\mathbf{x}},\n",
    "$$\n",
    "where $\\mathbf{S}$ is the positive-definite solution to the equation:\n",
    "$$\n",
    "0=\\mathbf{Q}-\\mathbf{S B R}^{-1} \\mathbf{B}^T \\mathbf{S}+\\mathbf{S A}+\\mathbf{A}^T \\mathbf{S}\n",
    "$$\n",
    "The optimal feedback policy for the linear system is given by\n",
    "$$\n",
    "\\overline{\\mathbf{u}}^*=-\\mathbf{R}^{-1} \\mathbf{B}^T \\mathbf{S} \\overline{\\mathbf{x}}=-\\mathbf{K} \\overline{\\mathbf{x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time-Invariant LQR Verification\n",
    "\n",
    "\\begin{aligned}\n",
    "\\operatorname{maximize} & \\rho, \\\\\n",
    "\\text { subject to } & \\hat{\\dot{J}}^*(\\overline{\\mathbf{x}})+h(\\overline{\\mathbf{x}})\\left(\\rho-\\hat{J}^*(\\overline{\\mathbf{x}})\\right) \\leq-\\epsilon\\|\\mathbf{x}\\|_2^2, \\\\\n",
    "& \\rho>0 \\\\\n",
    "& h(\\overline{\\mathbf{x}}) \\geq 0\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Trajectory Optimization\n",
    "\n",
    "We can perform well on even very complicated nonlinear systems. Given the nonlinear system $\\dot{\\mathbf{x}}=\\mathbf{f}(\\mathbf{x}, \\mathbf{u})$, we solve for a feasible trajectory of the system $\\mathbf{x}_0(t), \\mathbf{u}_0(t)$ over the finite time interval $\\left[t_0, t_f\\right]$ which (locally) optimizes a cost function of the form\n",
    "$$\n",
    "J=\\int_{t_0}^{t_f}\\left[1+\\mathbf{u}_0^T \\mathbf{R} \\mathbf{u}_0\\right] d t,\n",
    "$$\n",
    "often subject to a final value constraint (for instance, $\\mathbf{x}_0\\left(t_f\\right)=\\mathbf{x}_G$ )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time-Varying LQR\n",
    "\n",
    "Given a nominal trajectory, $\\mathbf{x}_0(t), \\mathbf{u}_0(t)$, over a finite time interval, $t \\in\\left[t_0, t_f\\right]$, we stabilize the trajectory using a time-varying LQR controller. Linearizing the system around the trajectory, we obtain:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\overline{\\mathbf{x}}(t)= & \\mathbf{x}(t)-\\mathbf{x}_0(t), \\quad \\overline{\\mathbf{u}}(t)=\\mathbf{u}(t)-\\mathbf{u}_0(t), \\\\\n",
    "& \\dot{\\overline{\\mathbf{x}}}(t) \\approx \\mathbf{A}(t) \\overline{\\mathbf{x}}(t)+\\mathbf{B}(t) \\overline{\\mathbf{u}}(t) .\n",
    "\\end{aligned}\n",
    "$$\n",
    "and define a quadratic regulator (tracking) cost function:\n",
    "$$\n",
    "\\begin{gathered}\n",
    "J\\left(\\overline{\\mathbf{x}}^{\\prime}, t^{\\prime}\\right)=\\overline{\\mathbf{x}}^T\\left(t_f\\right) \\mathbf{Q}_f \\overline{\\mathbf{x}}\\left(t_f\\right)+\\int_{t^{\\prime}}^{t_f}\\left[\\overline{\\mathbf{x}}^T(t) \\mathbf{Q} \\overline{\\mathbf{x}}(t)+\\overline{\\mathbf{u}}^T(t) \\mathbf{R} \\overline{\\mathbf{u}}(t)\\right] d t, \\\\\n",
    "\\mathbf{Q}_f=\\mathbf{Q}_f^T>\\mathbf{0}, \\mathbf{Q}=\\mathbf{Q}^T \\geq \\mathbf{0}, \\mathbf{R}=\\mathbf{R}^T>\\mathbf{0}, \\overline{\\mathbf{x}}\\left(t^{\\prime}\\right)=\\overline{\\mathbf{x}}^{\\prime}\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In general, $\\mathbf{Q}$ and $\\mathbf{R}$ could easily be made a function of time as well. With timevarying dynamics, the resulting cost-to-go is time-varying. It can be shown that the optimal cost-to-go, $J^*$, is given by\n",
    "$$\n",
    "J^*(\\overline{\\mathbf{x}}, t)=\\overline{\\mathbf{x}}^T \\mathbf{S}(t) \\overline{\\mathbf{x}}, \\quad \\mathbf{S}(t)=\\mathbf{S}^T(t)>\\mathbf{0} .\n",
    "$$\n",
    "where $\\mathbf{S}(t)$ is the solution to\n",
    "$$\n",
    "-\\dot{\\mathbf{S}}=\\mathbf{Q}-\\mathbf{S B R}^{-1} \\mathbf{B}^T \\mathbf{S}+\\mathbf{S A}+\\mathbf{A}^T \\mathbf{S}, \\quad \\mathbf{S}\\left(t_f\\right)=\\mathbf{Q}_f,\n",
    "$$\n",
    "and the optimal feedback policy is given by\n",
    "$$\n",
    "\\overline{\\mathbf{u}}^*(t)=-\\mathbf{R}^{-1} \\mathbf{B}^T(t) \\mathbf{S}(t) \\overline{\\mathbf{x}}(t)=-\\mathbf{K}(t) \\overline{\\mathbf{x}}(t) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LQR-Tree Algorithm\n",
    "\n",
    "The algorithm proceeds by producing a tree, $T$, with nodes containing the tuples, $\\left\\{\\mathbf{x}, \\mathbf{u}, \\mathbf{S}, \\mathbf{K}, \\rho_c, i\\right\\}$, \n",
    "\n",
    "where $J^*(\\overline{\\mathbf{x}}, t)=$ $\\overline{\\mathbf{x}}^T \\mathbf{S} \\overline{\\mathbf{x}}$ is the local quadratic approximation of the value function, $\\overline{\\mathbf{u}}^*=-\\mathbf{K} \\overline{\\mathbf{x}}$ is the feedback controller, $J^*(\\overline{\\mathbf{x}}, t) \\leq \\rho(t)$ is the funnel, \n",
    "\n",
    "$\\rho(t)$ is described by the vector of polynomial coefficients $\\rho_c$, and $i$ is a pointer to the parent node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Execution of the LQR-tree policy is accomplished by selecting any node in the tree with a basin of attraction which\n",
    "contains the initial conditions, $x(0)$, and following the timevarying feedback policy along that branch all of the way to\n",
    "the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Simulations\n",
    "\n",
    "The algorithm was tested on a simple pendulum:\n",
    "\n",
    "$I \\ddot{\\theta}+b \\dot{\\theta}+$ $m g l \\sin \\theta=\\tau$, \n",
    "\n",
    "with $m=1, l=.5, b=.1, I=m l^2, g=9.8$. Here $\\mathbf{x}=[\\theta, \\dot{\\theta}]^T$ and $\\mathbf{u}=\\tau$. \n",
    "\n",
    "The parameters of the LQR-tree algorithm were $\\mathbf{x}_G=[\\pi, 0]^T$, \n",
    "\n",
    "$\\mathbf{u}_G=0$, \n",
    "\n",
    "$\\mathbf{Q}=\\operatorname{diag}([10,1])$,\n",
    "\n",
    "$\\mathbf{R}=15$, \n",
    "\n",
    "$N_f=3, N_m=2, N_x=3, N_S=3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An LQR-tree for the simple pendulum. The $x$-axis is $\\theta \\in[-\\pi / 2,3 \\pi / 2]$ (note that the state wraps around this axis), and the $y$-axis is $\\dot{\\theta} \\in[-20,20]$. The green $X$ (on the left) represents the stable fixed point; the red $X$ (on the right) represents the unstable (upright) fixed point. The blue ovals represent the \"funnels,\" sampled at every node.\n",
    "\n",
    "![Funnels](./results/image_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Proposition 1: \n",
    "* For nonlinear systems described by a polynomial of degree $≤ N_f$ , the LQR-tree algorithm probabilistically covers the sampled portion of the reachable state space with a stabilizing controller and a Lyapunov function, thereby guaranteeing that all initial conditions which are capable of reaching the goal will stabilize to the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Controlling walking robots\n",
    "\n",
    "![Funnels](./results/image_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cart-Pole experiments 1\n",
    "\n",
    "![Cartpole funnels](results/cartpole_funnels_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cart-Pole experiments 2\n",
    "\n",
    "![Cartpole funnels](results/cartpole_funnels_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resources\n",
    "\n",
    "## Implementations for Cart-Pole experiments\n",
    "* [Repo](https://github.com/brooky56/MCP_LQR_Trees)\n",
    "\n",
    "Description how to use and try experiments with LQR-Tree feedback motion planning described in repo with brief sum-up of ideas behind this algorithm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Resources\n",
    "\n",
    "## Papers\n",
    "* [1] [LQR-Trees: Feedback Motion Planning via Sums-of-Squares Verification](https://www.researchgate.net/publication220122524_LQR-trees_Feedback_Motion_Planning_via_Sums-of-Squares_Verification) \n",
    "* [2] [Linear Quadratic Regulators](http://underactuated.mit.edu/lqr.html)\n",
    "* [3] [LQR-Trees: Feedback Motion Planning on Sparse Randomized Trees](https://groups.csail.mit.edu/robotics-center/public_papers/Tedrake09a.pdf)\n",
    "## Implementation\n",
    "* [1] [Pendulum Experiments](https://github.com/FedericoGirlanda/pendulumExp)\n",
    "* [2] [LQR trees](https://github.com/EpicDuckPotato/jubilant-octo-dollop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
